{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/erogol/97516ad65b44dbddb8cd694953187c5b/tts_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjD0xW0cEMVT"
   },
   "source": [
    "# Hands-on example for üê∏ [Coqui TTS](https://github.com/coqui-ai/TTS)\n",
    "\n",
    "This notebook trains Tacotron model on LJSpeech dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LJSpeech-1.1.tar.bz2', <http.client.HTTPMessage at 0x20804075a10>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"http://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
    "filename = \"LJSpeech-1.1.tar.bz2\"\n",
    "urllib.request.urlretrieve(url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(\"LJSpeech-1.1.tar.bz2\", \"r:bz2\") as tar:\n",
    "    tar.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPA2gbqRi9Wx"
   },
   "source": [
    "## Download LJSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XGiNTMShZYvj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# download LJSpeech dataset\n",
    "!wget http://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
    "# decompress\n",
    "!tar -xjf LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "__k0BrbfLQ-F"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'shuf' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'tail' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# create train-val splits\n",
    "!shuf LJSpeech-1.1/metadata.csv > LJSpeech-1.1/metadata_shuf.csv\n",
    "!head -n 12000 LJSpeech-1.1/metadata_shuf.csv > LJSpeech-1.1/metadata_train.csv\n",
    "!tail -n 1100 LJSpeech-1.1/metadata_shuf.csv > LJSpeech-1.1/metadata_val.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocmh66BqjLCF"
   },
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pyJwcU9pDUE-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting TTS\n",
      "  Downloading TTS-0.22.0.tar.gz (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.7/1.7 MB 9.9 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from TTS) (1.24.3)\n",
      "Collecting cython>=0.29.30 (from TTS)\n",
      "  Downloading Cython-3.0.11-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting scipy>=1.11.2 (from TTS)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting torch>=2.1 (from TTS)\n",
      "  Downloading torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchaudio (from TTS)\n",
      "  Downloading torchaudio-2.5.1-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting soundfile>=0.12.0 (from TTS)\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl.metadata (14 kB)\n",
      "Collecting librosa>=0.10.0 (from TTS)\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from TTS) (1.3.0)\n",
      "Requirement already satisfied: numba>=0.57.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from TTS) (0.57.1)\n",
      "Collecting inflect>=5.6.0 (from TTS)\n",
      "  Downloading inflect-7.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tqdm>=4.64.1 (from TTS)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting anyascii>=0.3.0 (from TTS)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pyyaml>=6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from TTS) (6.0)\n",
      "Collecting fsspec>=2023.6.0 (from TTS)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp>=3.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from TTS) (3.8.5)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from TTS) (23.1)\n",
      "Requirement already satisfied: flask>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from TTS) (2.2.2)\n",
      "Collecting pysbd>=0.3.4 (from TTS)\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting umap-learn>=0.5.1 (from TTS)\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pandas<2.0,>=1.4 (from TTS)\n",
      "  Downloading pandas-1.5.3-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from TTS) (3.7.2)\n",
      "Collecting trainer>=0.0.32 (from TTS)\n",
      "  Downloading trainer-0.0.36-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting coqpit>=0.0.16 (from TTS)\n",
      "  Downloading coqpit-0.0.17-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jieba (from TTS)\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "     ---------------------------------------- 0.0/19.2 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 3.1/19.2 MB 14.1 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 6.3/19.2 MB 14.8 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 9.2/19.2 MB 14.6 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 12.3/19.2 MB 14.8 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 14.7/19.2 MB 14.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 18.4/19.2 MB 14.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 19.2/19.2 MB 14.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pypinyin (from TTS)\n",
      "  Downloading pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting hangul_romanize (from TTS)\n",
      "  Downloading hangul_romanize-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3->TTS)\n",
      "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jamo (from TTS)\n",
      "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from TTS) (3.8.1)\n",
      "Collecting g2pkk>=0.1.1 (from TTS)\n",
      "  Downloading g2pkk-0.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting bangla (from TTS)\n",
      "  Downloading bangla-0.0.2-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting bnnumerizer (from TTS)\n",
      "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting bnunicodenormalizer (from TTS)\n",
      "  Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting einops>=0.6.0 (from TTS)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting transformers>=4.33.0 (from TTS)\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting encodec>=0.1.1 (from TTS)\n",
      "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
      "     ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "     ------------------------------ --------- 2.9/3.7 MB 13.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.7/3.7 MB 13.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting unidecode>=1.3.2 (from TTS)\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting num2words (from TTS)\n",
      "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting spacy>=3 (from spacy[ja]>=3->TTS)\n",
      "  Downloading spacy-3.8.3-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.11.0)\n",
      "Collecting dateparser~=1.1.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
      "  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Collecting gruut-ipa<1.0,>=0.12.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
      "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gruut_lang_en~=2.0.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
      "  Downloading gruut_lang_en-2.0.1.tar.gz (15.3 MB)\n",
      "     ---------------------------------------- 0.0/15.3 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 2.9/15.3 MB 15.2 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 6.0/15.3 MB 14.7 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 8.9/15.3 MB 14.6 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 11.8/15.3 MB 14.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.9/15.3 MB 14.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 15.3/15.3 MB 14.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonlines~=1.2.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
      "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting networkx<3.0.0,>=2.5.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
      "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting python-crfsuite~=0.9.7 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
      "  Downloading python_crfsuite-0.9.11-cp311-cp311-win_amd64.whl.metadata (4.4 kB)\n",
      "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
      "  Downloading gruut_lang_de-2.0.1.tar.gz (18.1 MB)\n",
      "     ---------------------------------------- 0.0/18.1 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.9/18.1 MB 13.9 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 5.8/18.1 MB 14.7 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 8.9/18.1 MB 14.6 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 12.1/18.1 MB 14.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 15.2/18.1 MB 14.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  17.8/18.1 MB 14.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 18.1/18.1 MB 14.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
      "  Downloading gruut_lang_es-2.0.1.tar.gz (31.4 MB)\n",
      "     ---------------------------------------- 0.0/31.4 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 2.9/31.4 MB 14.0 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 6.0/31.4 MB 14.8 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 9.2/31.4 MB 14.6 MB/s eta 0:00:02\n",
      "     -------------- ------------------------ 12.1/31.4 MB 14.8 MB/s eta 0:00:02\n",
      "     ------------------ -------------------- 15.2/31.4 MB 14.7 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 18.1/31.4 MB 14.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 21.2/31.4 MB 14.8 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 24.6/31.4 MB 14.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 27.5/31.4 MB 14.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 30.4/31.4 MB 14.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 31.4/31.4 MB 14.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
      "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
      "     ---------------------------------------- 0.0/10.9 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 2.9/10.9 MB 15.2 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 6.0/10.9 MB 14.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.9/10.9 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.9/10.9 MB 14.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp>=3.8.1->TTS) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp>=3.8.1->TTS) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp>=3.8.1->TTS) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp>=3.8.1->TTS) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp>=3.8.1->TTS) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp>=3.8.1->TTS) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp>=3.8.1->TTS) (1.2.0)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask>=2.0.1->TTS) (2.2.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask>=2.0.1->TTS) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask>=2.0.1->TTS) (2.0.1)\n",
      "Requirement already satisfied: click>=8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask>=2.0.1->TTS) (8.0.4)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from inflect>=5.6.0->TTS) (8.12.0)\n",
      "Collecting typeguard>=4.0.1 (from inflect>=5.6.0->TTS)\n",
      "  Downloading typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa>=0.10.0->TTS)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa>=0.10.0->TTS) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa>=0.10.0->TTS) (5.1.1)\n",
      "Collecting pooch>=1.1 (from librosa>=0.10.0->TTS)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa>=0.10.0->TTS)\n",
      "  Downloading soxr-0.5.0.post1-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\pratheek\\appdata\\roaming\\python\\python311\\site-packages (from librosa>=0.10.0->TTS) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa>=0.10.0->TTS) (0.2)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa>=0.10.0->TTS) (1.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->TTS) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->TTS) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->TTS) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->TTS) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->TTS) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->TTS) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\n",
      "Collecting docopt>=0.6.2 (from num2words->TTS)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba>=0.57.0->TTS) (0.40.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<2.0,>=1.4->TTS) (2023.3.post1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.3.0->TTS) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from soundfile>=0.12.0->TTS) (1.15.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading murmurhash-1.0.11-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading cymem-2.0.10-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading thinc-8.3.3-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading srsly-2.5.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\pratheek\\appdata\\roaming\\python\\python311\\site-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.10.8)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=3->spacy[ja]>=3->TTS) (68.0.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS)\n",
      "  Downloading SudachiPy-0.6.9-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting sudachidict_core>=20211220 (from spacy[ja]>=3->TTS)\n",
      "  Downloading SudachiDict_core-20241021-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.1->TTS) (3.9.0)\n",
      "Collecting sympy==1.13.1 (from torch>=2.1->TTS)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.1->TTS) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.64.1->TTS) (0.4.6)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from trainer>=0.0.32->TTS) (5.9.0)\n",
      "Collecting tensorboard (from trainer>=0.0.32->TTS)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers>=4.33.0->TTS)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.33.0->TTS) (2022.7.9)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.33.0->TTS)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.33.0->TTS)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.5.1->TTS)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.21)\n",
      "Collecting tzlocal (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Jinja2>=3.0->flask>=2.0.1->TTS) (2.1.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.16.0)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa>=0.10.0->TTS) (3.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2023.7.22)\n",
      "Collecting blis<1.2.0,>=1.1.0 (from thinc<8.4.0,>=8.3.0->spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading blis-1.1.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (5.2.1)\n",
      "Collecting absl-py>=0.4 (from tensorboard->trainer>=0.0.32->TTS)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->trainer>=0.0.32->TTS)\n",
      "  Downloading grpcio-1.68.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->trainer>=0.0.32->TTS) (3.4.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\pratheek\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard->trainer>=0.0.32->TTS) (4.25.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->trainer>=0.0.32->TTS)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: tzdata in c:\\programdata\\anaconda3\\lib\\site-packages (from tzlocal->dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2023.3)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "Downloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
      "Downloading Cython-3.0.11-cp311-cp311-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.8/2.8 MB 14.7 MB/s eta 0:00:00\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
      "Downloading inflect-7.4.0-py3-none-any.whl (34 kB)\n",
      "Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
      "Downloading pandas-1.5.3-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "   ---------------------------------------- 0.0/10.3 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 2.9/10.3 MB 14.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.0/10.3 MB 14.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.9/10.3 MB 14.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.3/10.3 MB 14.3 MB/s eta 0:00:00\n",
      "Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "Downloading scipy-1.14.1-cp311-cp311-win_amd64.whl (44.8 MB)\n",
      "   ---------------------------------------- 0.0/44.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 3.1/44.8 MB 15.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 6.0/44.8 MB 14.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 9.2/44.8 MB 15.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 12.3/44.8 MB 14.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 15.2/44.8 MB 14.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 18.6/44.8 MB 14.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 21.5/44.8 MB 14.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 24.4/44.8 MB 14.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 27.3/44.8 MB 14.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 30.4/44.8 MB 14.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 33.6/44.8 MB 14.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 36.7/44.8 MB 14.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.6/44.8 MB 14.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.7/44.8 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.8/44.8 MB 14.5 MB/s eta 0:00:00\n",
      "Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading spacy-3.8.3-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.9/12.2 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.0/12.2 MB 14.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.9/12.2 MB 15.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.2 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 14.1 MB/s eta 0:00:00\n",
      "Downloading torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 3.1/203.1 MB 14.2 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 5.2/203.1 MB 12.8 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 8.4/203.1 MB 13.3 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 11.3/203.1 MB 13.8 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 14.4/203.1 MB 13.9 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 17.6/203.1 MB 14.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 20.7/203.1 MB 14.2 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 23.9/203.1 MB 14.2 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 26.7/203.1 MB 14.4 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 29.9/203.1 MB 14.4 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 33.0/203.1 MB 14.5 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 35.9/203.1 MB 14.4 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 39.1/203.1 MB 14.5 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 41.9/203.1 MB 14.5 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 45.1/203.1 MB 14.6 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 48.2/203.1 MB 14.6 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 51.1/203.1 MB 14.6 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 54.3/203.1 MB 14.6 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 57.1/203.1 MB 14.6 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 60.3/203.1 MB 14.6 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 63.4/203.1 MB 14.6 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 66.6/203.1 MB 14.6 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 69.5/203.1 MB 14.6 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 72.4/203.1 MB 14.6 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 75.5/203.1 MB 14.6 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 78.4/203.1 MB 14.7 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 81.5/203.1 MB 14.7 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 84.4/203.1 MB 14.7 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 87.3/203.1 MB 14.7 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 90.4/203.1 MB 14.7 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 93.6/203.1 MB 14.7 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 96.7/203.1 MB 14.7 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 99.9/203.1 MB 14.7 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 102.8/203.1 MB 14.7 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 105.9/203.1 MB 14.7 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 109.1/203.1 MB 14.7 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 112.2/203.1 MB 14.7 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 115.1/203.1 MB 14.7 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 118.2/203.1 MB 14.7 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 121.1/203.1 MB 14.7 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 124.3/203.1 MB 14.7 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 127.1/203.1 MB 14.7 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 130.3/203.1 MB 14.7 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 131.6/203.1 MB 14.5 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 134.7/203.1 MB 14.5 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 137.9/203.1 MB 14.5 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 141.0/203.1 MB 14.6 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 143.9/203.1 MB 14.5 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 146.8/203.1 MB 14.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 149.9/203.1 MB 14.6 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 152.8/203.1 MB 14.6 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 156.0/203.1 MB 14.6 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 158.9/203.1 MB 14.6 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 162.0/203.1 MB 14.6 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 164.9/203.1 MB 14.6 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 168.0/203.1 MB 14.6 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 171.2/203.1 MB 14.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 174.3/203.1 MB 14.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 177.5/203.1 MB 14.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 180.6/203.1 MB 14.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 183.8/203.1 MB 14.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 186.9/203.1 MB 14.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 190.1/203.1 MB 14.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 192.9/203.1 MB 14.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 195.3/203.1 MB 14.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.4/203.1 MB 14.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  201.3/203.1 MB 14.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 14.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.1/203.1 MB 14.3 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 3.1/6.2 MB 14.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 14.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 12.2 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading trainer-0.0.36-py3-none-any.whl (51 kB)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 2.9/10.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.8/10.1 MB 14.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.1 MB 14.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 12.6 MB/s eta 0:00:00\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "Downloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
      "Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl (23 kB)\n",
      "Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n",
      "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n",
      "   ---------------------------------------- 0.0/834.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 834.7/834.7 kB 7.3 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.5.1-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------------------------------- - 2.4/2.4 MB 15.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.10-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
      "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.11-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 10.3 MB/s eta 0:00:00\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp311-cp311-win_amd64.whl (301 kB)\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "Downloading soxr-0.5.0.post1-cp311-cp311-win_amd64.whl (166 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.0-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading SudachiDict_core-20241021-py3-none-any.whl (72.1 MB)\n",
      "   ---------------------------------------- 0.0/72.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.9/72.1 MB 15.2 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 5.8/72.1 MB 14.7 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 8.9/72.1 MB 15.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 12.1/72.1 MB 14.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 14.9/72.1 MB 14.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 17.8/72.1 MB 14.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 21.0/72.1 MB 14.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 23.9/72.1 MB 14.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 27.0/72.1 MB 14.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 29.9/72.1 MB 14.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 32.8/72.1 MB 14.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 36.2/72.1 MB 14.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 39.1/72.1 MB 14.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 41.9/72.1 MB 14.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 45.1/72.1 MB 14.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 48.0/72.1 MB 14.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 51.1/72.1 MB 14.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 54.0/72.1 MB 14.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 57.1/72.1 MB 14.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 60.0/72.1 MB 14.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 63.2/72.1 MB 14.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 66.1/72.1 MB 14.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 68.9/72.1 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.1 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.1/72.1 MB 14.4 MB/s eta 0:00:00\n",
      "Downloading SudachiPy-0.6.9-cp311-cp311-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 7.9 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.3-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 8.8 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 13.6 MB/s eta 0:00:00\n",
      "Downloading typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 2.9/5.5 MB 15.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.5 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 12.9 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading blis-1.1.0-cp311-cp311-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 2.9/6.3 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.0/6.3 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 14.4 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading grpcio-1.68.1-cp311-cp311-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 2.9/4.4 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 12.6 MB/s eta 0:00:00\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 2.9/5.4 MB 15.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 11.7 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Building wheels for collected packages: TTS, gruut, encodec, bnnumerizer, jieba, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n",
      "  Building wheel for TTS (pyproject.toml): started\n",
      "  Building wheel for TTS (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for TTS: filename=TTS-0.22.0-cp311-cp311-win_amd64.whl size=904044 sha256=70b3ab4b44f64f509faffd15888f5fea69dbf923643ee1b476a1540cc566aaaa\n",
      "  Stored in directory: c:\\users\\pratheek\\appdata\\local\\pip\\cache\\wheels\\2a\\54\\92\\3ba43806fe20a11a7774184345f0e89498327b94503dc29bb1\n",
      "  Building wheel for gruut (setup.py): started\n",
      "  Building wheel for gruut (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75859 sha256=0585f7eb8e3644c60d75ce99217df0044dcdabe261fcca847420964d4ef7ed5a\n",
      "  Stored in directory: c:\\users\\pratheek\\appdata\\local\\pip\\cache\\wheels\\1f\\a0\\bc\\4dacab52579ab464cffafbe7a8e3792dd36ad9ac288b264843\n",
      "  Building wheel for encodec (setup.py): started\n",
      "  Building wheel for encodec (setup.py): finished with status 'done'\n",
      "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45806 sha256=e2cc98f1c775ffb05ad1fe563f664db6da1454e48456c1a8e61c4a59a1b0a14d\n",
      "  Stored in directory: c:\\users\\pratheek\\appdata\\local\\pip\\cache\\wheels\\b4\\a4\\88\\480018a664e58ca7ce6708759193ee51b017b3b72aa3df8a85\n",
      "  Building wheel for bnnumerizer (setup.py): started\n",
      "  Building wheel for bnnumerizer (setup.py): finished with status 'done'\n",
      "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5274 sha256=8b57ef0e8b431373fb6a1b204edfa22e037ad55d0e33c3bf0c1eefef0056f80f\n",
      "  Stored in directory: c:\\users\\pratheek\\appdata\\local\\pip\\cache\\wheels\\9e\\b9\\e3\\4145416693824818c0b931988a692676ecd4bbf2ea41d1eedd\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314474 sha256=5baa51254c4f29925361585b7a27eaaff3571fa7fbf8e6c2da339e979596abae\n",
      "  Stored in directory: c:\\users\\pratheek\\appdata\\local\\pip\\cache\\wheels\\ac\\60\\cf\\538a1f183409caf1fc136b5d2c2dee329001ef6da2c5084bef\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13775 sha256=8dde614667ffe207c8b9f0778a32fb2b9d87ab4c636ff34d13ee16aa4c9d83b5\n",
      "  Stored in directory: c:\\users\\pratheek\\appdata\\local\\pip\\cache\\wheels\\1a\\b0\\8c\\4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "  Building wheel for gruut-ipa (setup.py): started\n",
      "  Building wheel for gruut-ipa (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104908 sha256=9e26ff7efabda47eea3903eab2d6ab75ac15b576e6fd28102d07d6d30301b0e3\n",
      "  Stored in directory: c:\\users\\pratheek\\appdata\\local\\pip\\cache\\wheels\\c7\\10\\89\\a5908dd7a9a032229684b7679396785e19f816667f788087fb\n",
      "  Building wheel for gruut_lang_de (setup.py): started\n",
      "  Building wheel for gruut_lang_de (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.1-py3-none-any.whl size=18498318 sha256=c7931bbabf8abf7f998c718d28261b62bd8040c65338032d37d52f387f786353\n",
      "  Stored in directory: c:\\users\\pratheek\\appdata\\local\\pip\\cache\\wheels\\87\\fa\\df\\5fdf5d3cc26ba859b8698a1f28581d1a6aa081edc6df9847ab\n",
      "  Building wheel for gruut_lang_en (setup.py): started\n",
      "  Building wheel for gruut_lang_en (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.1-py3-none-any.whl size=15326864 sha256=83569c058d6c40288744d202afcdcbae068b981f1a2e74eb8c5932516f6e29cf\n",
      "  Stored in directory: c:\\users\\pratheek\\appdata\\local\\pip\\cache\\wheels\\06\\30\\52\\dc5cd222b4bbde285838fed1f96636e96f85cd75493e79a978\n",
      "  Building wheel for gruut_lang_es (setup.py): started\n",
      "  Building wheel for gruut_lang_es (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.1-py3-none-any.whl size=32173935 sha256=3d329ae85dde317242cf42b402c79f5d411400c07cd0865d31712f94156f91ec\n",
      "  Stored in directory: c:\\users\\pratheek\\appdata\\local\\pip\\cache\\wheels\\c8\\eb\\59\\30b5d15e56347e595f613036cbea0f807ad9621c75cd75d912\n",
      "  Building wheel for gruut_lang_fr (setup.py): started\n",
      "  Building wheel for gruut_lang_fr (setup.py): finished with status 'done'\n",
      "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968774 sha256=6a71a50ce068b6146ef0e8cd1e8baf64a9e10a181b1f1946eda41e31b2057ba0\n",
      "  Stored in directory: c:\\users\\pratheek\\appdata\\local\\pip\\cache\\wheels\\e0\\e7\\a0\\7c416a3eeaa94ca71bf7bcbc6289cced2263d8ba35e82444bb\n",
      "Successfully built TTS gruut encodec bnnumerizer jieba docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n",
      "Installing collected packages: sudachipy, jieba, jamo, hangul_romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, cymem, bnunicodenormalizer, bnnumerizer, bangla, wasabi, unidecode, tzlocal, typeguard, tqdm, tensorboard-data-server, sympy, sudachidict_core, spacy-loggers, spacy-legacy, soxr, scipy, safetensors, python-crfsuite, pysbd, pypinyin, num2words, networkx, murmurhash, marisa-trie, jsonlines, gruut-ipa, grpcio, fsspec, einops, cython, coqpit, cloudpathlib, catalogue, blis, audioread, anyascii, absl-py, torch, tensorboard, srsly, soundfile, preshed, pooch, pandas, language-data, inflect, huggingface-hub, dateparser, trainer, torchaudio, tokenizers, pynndescent, librosa, langcodes, gruut, g2pkk, confection, weasel, umap-learn, transformers, thinc, encodec, spacy, TTS\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.3\n",
      "    Uninstalling tqdm-4.62.3:\n",
      "      Successfully uninstalled tqdm-4.62.3\n",
      "Successfully installed TTS-0.22.0 absl-py-2.1.0 anyascii-0.3.2 audioread-3.0.1 bangla-0.0.2 blis-1.1.0 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.7 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 coqpit-0.0.17 cymem-2.0.10 cython-3.0.11 dateparser-1.1.8 docopt-0.6.2 einops-0.8.0 encodec-0.1.1 fsspec-2024.10.0 g2pkk-0.1.2 grpcio-1.68.1 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.1 gruut_lang_en-2.0.1 gruut_lang_es-2.0.1 gruut_lang_fr-2.0.2 hangul_romanize-0.1.0 huggingface-hub-0.27.0 inflect-7.4.0 jamo-0.4.1 jieba-0.42.1 jsonlines-1.2.0 langcodes-3.5.0 language-data-1.3.0 librosa-0.10.2.post1 marisa-trie-1.2.1 murmurhash-1.0.11 networkx-2.8.8 num2words-0.5.14 pandas-1.5.3 pooch-1.8.2 preshed-3.0.9 pynndescent-0.5.13 pypinyin-0.53.0 pysbd-0.3.4 python-crfsuite-0.9.11 safetensors-0.4.5 scipy-1.14.1 soundfile-0.12.1 soxr-0.5.0.post1 spacy-3.8.3 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.0 sudachidict_core-20241021 sudachipy-0.6.9 sympy-1.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 thinc-8.3.3 tokenizers-0.21.0 torch-2.5.1 torchaudio-2.5.1 tqdm-4.67.1 trainer-0.0.36 transformers-4.47.1 typeguard-4.4.1 tzlocal-5.2 umap-learn-0.5.7 unidecode-1.3.8 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script sudachipy.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script unidecode.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pypinyin.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script gruut-ipa.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe, torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script dateparser-download.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script gruut.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script weasel.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script encodec.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts tts-server.exe and tts.exe are installed in 'C:\\Users\\Pratheek\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "s3fs 2023.4.0 requires fsspec==2023.4.0, but you have fsspec 2024.10.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install TTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zV-vHTWyirQv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# install espeak backend if you like to use phonemes instead of raw characters\n",
    "!apt-get install espeak-ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Af-yiyFjU-f"
   },
   "source": [
    "## Train Tacotron DCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y7_Xao7uNOvX"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TTS.tts.configs.datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainerArgs\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared_configs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseAudioConfig\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LJSpeechDatasetConfig  \u001b[38;5;66;03m# Updated import\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtacotron2_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tacotron2Config\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_tts_samples\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'TTS.tts.configs.datasets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.config.shared_configs import BaseAudioConfig\n",
    "from TTS.tts.configs.datasets import LJSpeechDatasetConfig  # Updated import\n",
    "from TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.tacotron2 import Tacotron2\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "\n",
    "output_path = \"./\"\n",
    "\n",
    "# Use the dataset-specific config class\n",
    "dataset_config = LJSpeechDatasetConfig(\n",
    "    meta_file_train=\"metadata.csv\",\n",
    "    path=os.path.join(output_path, \"content/LJSpeech-1.1\")\n",
    ")\n",
    "\n",
    "audio_config = BaseAudioConfig(\n",
    "    sample_rate=22050,\n",
    "    do_trim_silence=True,\n",
    "    trim_db=60.0,\n",
    "    signal_norm=False,\n",
    "    mel_fmin=0.0,\n",
    "    mel_fmax=8000,\n",
    "    spec_gain=1.0,\n",
    "    log_func=\"np.log\",\n",
    "    ref_level_db=20,\n",
    "    preemphasis=0.0,\n",
    ")\n",
    "\n",
    "config = Tacotron2Config(\n",
    "    audio=audio_config,\n",
    "    batch_size=64,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    ga_alpha=0.0,\n",
    "    decoder_loss_alpha=0.25,\n",
    "    postnet_loss_alpha=0.25,\n",
    "    postnet_diff_spec_alpha=0,\n",
    "    decoder_diff_spec_alpha=0,\n",
    "    decoder_ssim_alpha=0,\n",
    "    postnet_ssim_alpha=0,\n",
    "    r=2,\n",
    "    attention_type=\"dynamic_convolution\",\n",
    "    double_decoder_consistency=False,\n",
    "    epochs=1000,\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"en-us\",\n",
    "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
    "    print_step=25,\n",
    "    print_eval=True,\n",
    "    mixed_precision=False,\n",
    "    output_path=output_path,\n",
    "    datasets=[dataset_config],\n",
    ")\n",
    "\n",
    "ap = AudioProcessor.init_from_config(config)\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=True,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    ")\n",
    "\n",
    "model = Tacotron2(config, ap, tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(),\n",
    "    config,\n",
    "    output_path,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples\n",
    ")\n",
    "\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.0\n"
     ]
    }
   ],
   "source": [
    "import TTS\n",
    "print(TTS.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AudioProcessor', 'Callable', 'Counter', 'Dataset', 'Dict', 'ET', 'EnergyDataset', 'F0Dataset', 'List', 'Path', 'PhonemeDataset', 'TTSDataset', 'Tuple', 'Union', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_get_formatter_by_name', 'add_extra_keys', 'baker', 'base64', 'bel_tts_formatter', 'brspeech', 'calculate_energy', 'cml_tts', 'collections', 'common_voice', 'coqui', 'css10', 'custom_turkish', 'dataset', 'emotion', 'find_unique_chars', 'formatters', 'glob', 'kokoro', 'kss', 'libri_tts', 'ljspeech', 'ljspeech_test', 'load_attention_mask_meta_data', 'load_tts_samples', 'mailabs', 'mls', 'mozilla', 'mozilla_de', 'nancy', 'noise_augment_audio', 'np', 'open_bible', 'os', 'pd', 'prepare_data', 'prepare_stop_target', 'prepare_tensor', 'random', 're', 'ruslan', 'sam_accenture', 'split_dataset', 'string2filename', 'synpaflex', 'sys', 'thorsten', 'torch', 'tqdm', 'tweb', 'vctk', 'vctk_old', 'voxceleb1', 'voxceleb2']\n"
     ]
    }
   ],
   "source": [
    "import TTS.tts.datasets\n",
    "print(dir(TTS.tts.datasets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.tts.datasets import ljspeech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TTS.tts.datasets.ljspeech'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mljspeech\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mljspeech_module\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mdir\u001b[39m(ljspeech_module))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'TTS.tts.datasets.ljspeech'"
     ]
    }
   ],
   "source": [
    "import TTS.tts.datasets.ljspeech as ljspeech_module\n",
    "print(dir(ljspeech_module))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TTS.tts.datasets.ljspeech'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mljspeech\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LJSpeech\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'TTS.tts.datasets.ljspeech'"
     ]
    }
   ],
   "source": [
    "from TTS.tts.datasets.ljspeech import LJSpeech  # Adjust the class name if different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTS Library Version: 0.22.0\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60.0\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './content/LJSpeech-1.1\\\\metadata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ljspeech  \u001b[38;5;66;03m# Directly use the imported ljspeech function\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Load Training and Evaluation Samples\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m train_samples, eval_samples \u001b[38;5;241m=\u001b[39m load_tts_samples(\n\u001b[0;32m     77\u001b[0m     dataset_config,\n\u001b[0;32m     78\u001b[0m     eval_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     79\u001b[0m     eval_split_max_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39meval_split_max_size,\n\u001b[0;32m     80\u001b[0m     eval_split_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39meval_split_size,\n\u001b[0;32m     81\u001b[0m     formatter\u001b[38;5;241m=\u001b[39mformatter  \u001b[38;5;66;03m# Pass the formatter function directly\u001b[39;00m\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Initialize the Tacotron2 Model\u001b[39;00m\n\u001b[0;32m     85\u001b[0m model \u001b[38;5;241m=\u001b[39m Tacotron2(config, ap, tokenizer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\TTS\\tts\\datasets\\__init__.py:120\u001b[0m, in \u001b[0;36mload_tts_samples\u001b[1;34m(datasets, eval_split, formatter, eval_split_max_size, eval_split_size)\u001b[0m\n\u001b[0;32m    118\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m _get_formatter_by_name(formatter_name)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# load train set\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m meta_data_train \u001b[38;5;241m=\u001b[39m formatter(root_path, meta_file_train, ignored_speakers\u001b[38;5;241m=\u001b[39mignored_speakers)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(meta_data_train) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [!] No training samples found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta_file_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m meta_data_train \u001b[38;5;241m=\u001b[39m add_extra_keys(meta_data_train, language, dataset_name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\TTS\\tts\\datasets\\formatters.py:197\u001b[0m, in \u001b[0;36mljspeech\u001b[1;34m(root_path, meta_file, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m items \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    196\u001b[0m speaker_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mljspeech\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(txt_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m ttf:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m ttf:\n\u001b[0;32m    199\u001b[0m         cols \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './content/LJSpeech-1.1\\\\metadata.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.config.shared_configs import BaseAudioConfig\n",
    "from TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
    "from TTS.tts.datasets import load_tts_samples, ljspeech  # Directly import ljspeech formatter\n",
    "from TTS.tts.models.tacotron2 import Tacotron2\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "\n",
    "# Optional: Verify TTS Library Version\n",
    "import TTS\n",
    "print(f\"TTS Library Version: {TTS.__version__}\")\n",
    "\n",
    "output_path = \"./\"\n",
    "\n",
    "# Define the dataset configuration without the 'name' parameter\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    meta_file_train=\"metadata.csv\",\n",
    "    path=os.path.join(output_path, \"content/LJSpeech-1.1\")\n",
    ")\n",
    "\n",
    "# Configure audio processing parameters\n",
    "audio_config = BaseAudioConfig(\n",
    "    sample_rate=22050,\n",
    "    do_trim_silence=True,\n",
    "    trim_db=60.0,\n",
    "    signal_norm=False,\n",
    "    mel_fmin=0.0,\n",
    "    mel_fmax=8000,\n",
    "    spec_gain=1.0,\n",
    "    log_func=\"np.log\",\n",
    "    ref_level_db=20,\n",
    "    preemphasis=0.0,\n",
    ")\n",
    "\n",
    "# Configure the Tacotron2 model\n",
    "config = Tacotron2Config(\n",
    "    audio=audio_config,\n",
    "    batch_size=64,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    ga_alpha=0.0,\n",
    "    decoder_loss_alpha=0.25,\n",
    "    postnet_loss_alpha=0.25,\n",
    "    postnet_diff_spec_alpha=0,\n",
    "    decoder_diff_spec_alpha=0,\n",
    "    decoder_ssim_alpha=0,\n",
    "    postnet_ssim_alpha=0,\n",
    "    r=2,\n",
    "    attention_type=\"dynamic_convolution\",\n",
    "    double_decoder_consistency=False,\n",
    "    epochs=1000,\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"en-us\",\n",
    "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
    "    print_step=25,\n",
    "    print_eval=True,\n",
    "    mixed_precision=False,\n",
    "    output_path=output_path,\n",
    "    datasets=[dataset_config],\n",
    ")\n",
    "\n",
    "# Initialize Audio Processor and Tokenizer\n",
    "ap = AudioProcessor.init_from_config(config)\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "\n",
    "# Assign the LJSpeech Formatter\n",
    "formatter = ljspeech  # Directly use the imported ljspeech function\n",
    "\n",
    "# Load Training and Evaluation Samples\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=True,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    "    formatter=formatter  # Pass the formatter function directly\n",
    ")\n",
    "\n",
    "# Initialize the Tacotron2 Model\n",
    "model = Tacotron2(config, ap, tokenizer)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(),\n",
    "    config,\n",
    "    output_path,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTS Library Version: 0.22.0\n",
      "Output Path: C:\\Users\\Pratheek\\Desktop\\nlp\\tts\n",
      "Dataset Path: C:\\Users\\Pratheek\\Desktop\\nlp\\tts\\content\\LJSpeech-1.1\n",
      "Does dataset path exist? False\n",
      "Metadata Path: C:\\Users\\Pratheek\\Desktop\\nlp\\tts\\content\\LJSpeech-1.1\\metadata.csv\n",
      "Does metadata.csv exist? False\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60.0\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      "Error loading samples: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Pratheek\\\\Desktop\\\\nlp\\\\tts\\\\content\\\\LJSpeech-1.1\\\\metadata.csv'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 87\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     train_samples, eval_samples \u001b[38;5;241m=\u001b[39m load_tts_samples(\n\u001b[0;32m     88\u001b[0m         dataset_config,\n\u001b[0;32m     89\u001b[0m         eval_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     90\u001b[0m         eval_split_max_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39meval_split_max_size,\n\u001b[0;32m     91\u001b[0m         eval_split_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39meval_split_size,\n\u001b[0;32m     92\u001b[0m         formatter\u001b[38;5;241m=\u001b[39mformatter  \u001b[38;5;66;03m# Pass the formatter function directly\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\TTS\\tts\\datasets\\__init__.py:120\u001b[0m, in \u001b[0;36mload_tts_samples\u001b[1;34m(datasets, eval_split, formatter, eval_split_max_size, eval_split_size)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# load train set\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m meta_data_train \u001b[38;5;241m=\u001b[39m formatter(root_path, meta_file_train, ignored_speakers\u001b[38;5;241m=\u001b[39mignored_speakers)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(meta_data_train) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [!] No training samples found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta_file_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\TTS\\tts\\datasets\\formatters.py:197\u001b[0m, in \u001b[0;36mljspeech\u001b[1;34m(root_path, meta_file, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m speaker_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mljspeech\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(txt_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m ttf:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m ttf:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Pratheek\\\\Desktop\\\\nlp\\\\tts\\\\content\\\\LJSpeech-1.1\\\\metadata.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[16], line 100\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Initialize the Tacotron2 Model\u001b[39;00m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2097\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2095\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2096\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2097\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mInteractiveTB\u001b[38;5;241m.\u001b[39mget_exception_only(etype,\n\u001b[0;32m   2098\u001b[0m                                                      value))\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2101\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m    569\u001b[0m             etype,\n\u001b[0;32m    570\u001b[0m             evalue,\n\u001b[0;32m    571\u001b[0m             (etb, chained_exc_ids),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    572\u001b[0m             chained_exceptions_tb_offset,\n\u001b[0;32m    573\u001b[0m             context,\n\u001b[0;32m    574\u001b[0m         )\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FormattedTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;28mself\u001b[39m, etype, evalue, etb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1437\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1323\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VerboseTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1327\u001b[0m         \u001b[38;5;28mself\u001b[39m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1328\u001b[0m     )\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1166\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1170\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1171\u001b[0m ):\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m   1174\u001b[0m                                                            tb_offset)\n\u001b[0;32m   1176\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1064\u001b[0m )\n\u001b[0;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1067\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1131\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(cf\u001b[38;5;241m.\u001b[39mtb_frame)\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.config.shared_configs import BaseAudioConfig\n",
    "from TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
    "from TTS.tts.datasets import load_tts_samples, ljspeech  # Directly import ljspeech formatter\n",
    "from TTS.tts.models.tacotron2 import Tacotron2\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "\n",
    "# Optional: Verify TTS Library Version\n",
    "import TTS\n",
    "print(f\"TTS Library Version: {TTS.__version__}\")\n",
    "\n",
    "output_path = os.path.abspath(\"./\")  # Use absolute path\n",
    "print(f\"Output Path: {output_path}\")\n",
    "\n",
    "# Define the dataset configuration\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    meta_file_train=\"metadata.csv\",\n",
    "    path=os.path.join(output_path, \"content\", \"LJSpeech-1.1\")\n",
    ")\n",
    "\n",
    "# Verify that the dataset path exists\n",
    "print(f\"Dataset Path: {dataset_config.path}\")\n",
    "print(f\"Does dataset path exist? {os.path.exists(dataset_config.path)}\")\n",
    "\n",
    "# Define the path to metadata.csv\n",
    "metadata_path = os.path.join(dataset_config.path, dataset_config.meta_file_train)\n",
    "print(f\"Metadata Path: {metadata_path}\")\n",
    "print(f\"Does metadata.csv exist? {os.path.exists(metadata_path)}\")\n",
    "\n",
    "# Configure audio processing parameters\n",
    "audio_config = BaseAudioConfig(\n",
    "    sample_rate=22050,\n",
    "    do_trim_silence=True,\n",
    "    trim_db=60.0,\n",
    "    signal_norm=False,\n",
    "    mel_fmin=0.0,\n",
    "    mel_fmax=8000,\n",
    "    spec_gain=1.0,\n",
    "    log_func=\"np.log\",\n",
    "    ref_level_db=20,\n",
    "    preemphasis=0.0,\n",
    ")\n",
    "\n",
    "# Configure the Tacotron2 model\n",
    "config = Tacotron2Config(\n",
    "    audio=audio_config,\n",
    "    batch_size=64,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    ga_alpha=0.0,\n",
    "    decoder_loss_alpha=0.25,\n",
    "    postnet_loss_alpha=0.25,\n",
    "    postnet_diff_spec_alpha=0,\n",
    "    decoder_diff_spec_alpha=0,\n",
    "    decoder_ssim_alpha=0,\n",
    "    postnet_ssim_alpha=0,\n",
    "    r=2,\n",
    "    attention_type=\"dynamic_convolution\",\n",
    "    double_decoder_consistency=False,\n",
    "    epochs=1000,\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"en-us\",\n",
    "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
    "    print_step=25,\n",
    "    print_eval=True,\n",
    "    mixed_precision=False,\n",
    "    output_path=output_path,\n",
    "    datasets=[dataset_config],\n",
    ")\n",
    "\n",
    "# Initialize Audio Processor and Tokenizer\n",
    "ap = AudioProcessor.init_from_config(config)\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "\n",
    "# Assign the LJSpeech Formatter\n",
    "formatter = ljspeech  # Directly use the imported ljspeech function\n",
    "\n",
    "# Load Training and Evaluation Samples\n",
    "try:\n",
    "    train_samples, eval_samples = load_tts_samples(\n",
    "        dataset_config,\n",
    "        eval_split=True,\n",
    "        eval_split_max_size=config.eval_split_max_size,\n",
    "        eval_split_size=config.eval_split_size,\n",
    "        formatter=formatter  # Pass the formatter function directly\n",
    "    )\n",
    "    print(f\"Number of training samples: {len(train_samples)}\")\n",
    "    print(f\"Number of evaluation samples: {len(eval_samples)}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading samples: {e}\")\n",
    "    # Optionally, exit or handle the error appropriately\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize the Tacotron2 Model\n",
    "model = Tacotron2(config, ap, tokenizer)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(),\n",
    "    config,\n",
    "    output_path,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TTS_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
